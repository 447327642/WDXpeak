# Machine Learning for Recommender Systems

<!-- MarkdownTOC -->

- The "Recommender Problem"
- A good recommendation
- Approaches to Recommendation
- What works?
- Collaborative Filtering
    - Performance Implications
    - Memory based CF
    - The Sparsity Problem
    - Personalised vs Non-Personalised CF
- Model Based CF Algorithms
    - Restricted Boltzmann Machines
    - Learning to Rank
    - Recurrent Neural Networks for CF
    - Clustering
    - Association rules
- Limitations of CF

<!-- /MarkdownTOC -->


## The "Recommender Problem"

Estimate a **utility function** to **predict** how a user will **like** an item.

C := {users}

S := {recommendable items}

u := utility function, measures the usefulness of item s to∈ user c

    u : C x S -> R

    where R:={recommended items}

For each user c, we want to choose the items s that maximize

∀c ∈ C, s~c~' = argmax(u(c, s)) s ∈ S

## A good recommendation

+ is relevant to the user
+ is diverse: represents all the possible interests of one user
+ Does not recommend items the user already knows or would have found anyway
+ Expands the user's taste into neighboring areas. **Serendipity** = Unsought finding

## Approaches to Recommendation

Recommendations are based on:

+ Users' **past** behavior(Collaborative Filtering - CF):
    + Find similar users to me and recommend what they liked(**Momory-based**)
    + Model taste patterns in large amounts of user-item interactions(**Model-based)
+ Item features(Content-based)
+ Users' features(Demographic)
+ Social recommendations(Trust-based)

## What works?

+ Depends on the **domain** and particular problem
+ Currently, the best approach is **Collaborative Filtering**
+ Other approaches can be combined to improve results
+ What matters?
    + **Data preprocessing**: outlier removal, denoising, removal of global effects
    + **Combining methods**

## Collaborative Filtering

The task of **predicting**(filtering) user preferences on new items by **collecting** taste information from many users(collaborative).

Challenges:

+ many items to choose from
+ very few recommendations to propose
+ few data per user
+ no data for new user
+ very large database

用矩阵表示的话，会很稀疏，稀疏矩阵的表示与计算

### Performance Implications

+ User-based similarity is more **dynamic**. Precomputing user neighbourhood can lead to poor predictions.
+ Item-based similarity is **static**. We can precompute item neighbourhood. Online computation of the predicted ratings.

### Memory based CF

Pros:

+ Requires minimal knowledge engineering efforts
+ Users and products are symbols without any internal structure or characteristics
+ Produces good-enough results in most cases

Cons:

+ Requires a large number of explicit and reliable "ratings"
+ Requires standardized products: users should have bought exactly the same product
+ Assumes that prior behaviour determines current hehaviour without taking into account "contextual" knowledge

### The Sparsity Problem

+ Typically large product sets & few user ratings
+ CF must have a number of users ~ 10% of the product catalogue size

Methods for dimensionality reduction

+ Matrix Factorization
+ Clustering
+ Projection(Principal Component Analysis)

### Personalised vs Non-Personalised CF

+ CF recommendations are **personalized**: the prediction is based on the ratings expressed by **similar users**; **neighbours** are **different** for each target user.
+ A **non-personalized** collaborative-based recommendation can be generated by averaging the recommendations of **ALL** users
+ How would the two approaches compare?

Use Mean Average Error(MAE)

## Model Based CF Algorithms

Models are learned from the underlying data rather than heuristics.

Develop a model of user ratings:

+ Bayesian network (probabilistic)
+ Clustering (classification)
+ Association rules
+ Other models:
    + linear regression
    + probabilistic latent semantic analysis

### Restricted Boltzmann Machines

+ A (generative stochastic) Neural Network
+ Learns a probability distribution over its inputs
+ Used in dimensionality reduction, CF, topic modeling, feature learning
+ Essential components of Deep Learning methods (DBN's, DBM's)

May need further investigation

+ Each unit is in a state which can be active or not active
+ Each input of a unit is associated to a weight
+ The transfer function calculates for each unit a score based on the weighted sum of the inputs
+ This score is passed to the activation function which calculated the probability that the unit state is active

### Learning to Rank

+ Usually we care to make accurate ranking and not rating prediction
+ Square loss optimizes to accurately predict 1s and 5s
+ RS should get the top items right -> Ranking problem
+ Why not to learn how to rank directly
+ Learning to Rank methods provide up to 30% performance improvements in off-line evaluation

### Recurrent Neural Networks for CF

+ RNN's are Neural Networks designed to model sequences
+ RNN for CF tries to predict the next item given all previous ones
+ RNN's model the current output as a function of the previous ouput and a hidden (latent) state
+ RNN models the items

### Clustering

+ Cluster customers into categories based on preferences & past purchases
+ Compute recommendations at the cluster level: all customers within a cluster receive the same recommendations

### Association rules

Pros:

+ Fast to implement
+ Fast to execute
+ Not much storage space required
+ Not **indiviudal** specific
+ Very successful in broad applications for large populations, such as shelf layout in retail stores

Cons:

+ Not suitable if preferences change rapidly
+ Rules can be used only when enough data validates them. False associations can arise

## Limitations of CF

+ Cold Start:
    + It needs to have enough users in the system
    + New items need to get enough ratings
+ Sparsity:
    + It is hard to find users who rated the same items
+ Popularity Bias:
    + Cannot recommend items to users with unique tastes
    + Tends to recommend popular items

