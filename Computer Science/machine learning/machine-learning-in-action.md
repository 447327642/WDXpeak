# 机器学习实战

<!-- MarkdownTOC -->

- 第一部分 分类
- 第 1 章 机器学习基础
    - 如何选择合适的算法
    - 开发机器学习应用程序的步骤
- 第 2 章 k-近邻算法(kNN)
    - k-近邻算法的一般流程
    - 使用k-近邻算法改进约会网站的配对效果
        - 使用 Matplotlib 创建散点图
        - 归一化数值
        - 测试算法
    - 小结
- 第 3 章 决策树
    - 决策树的一般流程
    - 小结
- 第 4 章 基于概率论的分类方法：朴素贝叶斯
    - 朴素贝叶斯的一般过程
    - 文档词袋模型
    - 过滤垃圾电子邮件
    - 小结
- 第 5 章 Logistic 回归
    - Logitstic 回归的一般过程

<!-- /MarkdownTOC -->


## 第一部分 分类

本书前两部分主要谈好监督学习(supervised learning)。在监督学习过程中，我们只需要给定输入样本集，机器就可以 从中推演出指定目标变量的可能结果。监督学习相对比较简单，机器只需从输入数据中预测合适的模型，并从中计算出目标变量的结果。

监督学习一般使用两种类型的目标变量：标称型和数值型。标称型目标变量的结果只在有限目标集中取值，如真与假。数值型目标变量主要用于回归分析。

## 第 1 章 机器学习基础

机器学习：利用计算机来彰显数据背后的真实含义。

简单地说，机器学习就是把无序的数据转换成有用的信息。

机器学习的主要任务就是**分类**。最终我们决定使用某个机器学习算法进行分类，首先需要做的是算法训练，即学习如何分类。通常我们为算法输入大量已分类数据作为算法的**训练集**。训练集是用于训练机器学习算法的数据样本集合，其中包含若干**训练样本**，每个训练样本会有若干种特征以及一个**目标变量**。目标变量是机器学习算法的预测结果，在分类算法中目标变量的类型通常是标称型的，而在回归算法中通常是连续型的。训练样本集必须确定知道目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。

我们通常将分类问题中的目标变量称为**类别**，并假定分类问题只存在有限个数的类别。

![mla1.2](./_resources/mla1.2.jpg)

> 特征或者属性通常是训练样本集的列，它们是独立测量得到的结果，多个特征联系在一起共同组成一个训练样本

为了测试机器学习算法的效果，通常使用两套独立的样本集：训练数据和**测试数据**。当机器学习程序开始运行时，使用训练样本集作为算法的输入，训练完成之后输入测试样本。比较测试样本预测的目标变量值域实际样本类别之间的差别，就可以得出算法的实际精确度。

机器学习的另一项任务是**回归**，它主要用于预测数值型数据。例子：数据拟合曲线，通过给定数据点的最优拟合曲线。分类和回归属于**监督学习**，之所以称之为监督学习，是因为这类算法必须知道预测什么，即目标变量的分类信息。

与监督学习相对于的是**无监督学习**，此时数据没有类别信息，也不会给定目标值。在无监督学习中，将数据集合分成由类似的对象组成的多个类的过程被称为**聚类**；将寻找描述数据统计值的过程称之为**密度估计**。此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。

监督学习 | 用途
--- | ---
k-近邻算法 | 线性回归
朴素贝叶斯算法 | 局部加权线性回归
支持向量机 | Ridge 回归
决策树 | Lasso 最小回归系数估计

无监督学习 | 用途
--- | ---
k-均值 | 最大期望算法
DBSCAN | Parzen 窗设计

### 如何选择合适的算法

首先考虑使用机器学习算法的目的。如果想要预测目标变量的值，则可以选择监督学习算法，否则可以选择无监督学习算法。确定选择监督学习算法之后，需要进一步确定目标变量的类型，如果目标变量是离散型，则可以选择分类器算法；如果目标变量是连续型的数值，则需要选择回归算法。

如果不想预测目标变量的值，则可以选择无监督学习算法。进一步分析是否需要将数据划分为离散的组。如果这是唯一的需求，则使用聚类算法；如果还需要估计数据与每个分组的相似程度，则需要使用密度估计算法。

其次需要考虑的是数据问题。对实际数据了解得越充分，越容易创建符合实际需求的应用程序。主要应该了解数据的以下特性：特征值是离散型变量还是连续型变量，特征值中是否存在缺失的值，何种原因造成缺失值，数据中是否存在异常值，某个特征发生的概率如何。

一般来说，发现最好算法的关键环节是反复试错的迭代过程。

### 开发机器学习应用程序的步骤

1. **收集数据**。爬虫、RSS、API、设备数据。公开可用的数据源
2. **准备输入数据**。确保数据格式符合要求
3. **分析输入数据**。图形化展示
4. **训练算法**。抽取知识或信息
5. **测试算法**。评估效果
6. **使用算法**。将算法转换为应用程序

## 第 2 章 k-近邻算法(kNN)

简单来说，k-近邻算法采用测量不同特征值之间的距离方法进行分类。

+ 优点：精度高、对异常值不敏感、无数据输入假定
+ 缺点：计算复杂度高、空间复杂度高
+ 适用数据范围：数值型和标称型

工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中对应的特征进行比较，然后算法提取样本集中前 k 个最相似的数据。最后，选择 k 个最相似数据中出现次数最多的分类，作为新数据的分类。

### k-近邻算法的一般流程

1. 收集数据：可以使用任何方法
2. 准备数据：距离计算所需要的数值，最好是结构化的数据格式
3. 分析数据：可以使用任何方法
4. 训练算法：此步骤不适用于k-近邻算法
5. 测试算法：计算错误率
6. 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理

对未知类别属性的数据集中的每个点依次执行以下操作：

1. 计算已知类别数据集中的点与当前点之间的距离；
2. 按照距离递增次序排序；
3. 选取与当前点距离最小的 k 个点；
4. 确定前 k 个点所在类别的出现频率
5. 返回前 k 个点出现频率最高的类别作为当前点的预测分类

具体可以参考 `AlgorithmTour/MachineLearningInAction/kNN.py`

    import kNN
    group, labels = kNN.createDataSet()
    kNN.classify0([0,0], group, labels, 3)

### 使用k-近邻算法改进约会网站的配对效果

海伦收集了一些约会数据，包括：

+ 每年获得的飞行常客里程数
+ 玩视频游戏所耗时间百分比
+ 每周消费的冰激凌公升数

使用 `file2matrix` 函数把数据 `datingTestSet.txt` 读入到程序中

    reload(kNN)
    datingDataMat, datingLabels = kNN.file2matrix('datingTestSet2.txt')

#### 使用 Matplotlib 创建散点图

在 Python 命令行环境中

    import matplotlib
    import matplotlib.pyplot as plt
    from numpy import array
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(datingDataMat[:,1], datingDataMat[:,2])
    plt.show()

得到下图

![mla2.3](./_resources/mla2.3.jpg)

乱七八糟完全看不清，使用 `scatter` 函数利用标签来个性化标记

    ax.scatter(datingDataMat[:,1], datingDataMat[:,2], 15.0*array(datingLabels), 15.0*array(datingLabels))
    plt.show()

标上颜色之后，效果会好很多，可是依然很难从中获得什么有效的信息

![mla2.4](./_resources/mla2.4.jpg)

(以上两张图的横轴是玩视频游戏所耗时间百分比，总州市每周消费的冰淇淋公升数)

如果把横轴和纵轴换成每年获取的飞行常客里程数和玩视频游戏所耗时间百分比，结论会更清晰一些

    ax.scatter(datingDataMat[:,0], datingDataMat[:,1], 15.0*array(datingLabels), 15.0*array(datingLabels))
    plt.show()

![mla2.5](./_resources/mla2.5.jpg)

#### 归一化数值

为了避免不同的特征的数值不同所导致的影响不同，可能需要进行归一化，也就是把特征值转换成[0,1]值

#### 测试算法

机器学习算法一个很重要的工作就是评估算法的正确率，通常我们只提供已有数据的 90% 作为训练样本来训练分类器，而使用剩余的 10% 数据去测试分类器，检测分类器的正确率

之后是手写的另一个例子，就略过了，总体思想是差不多的。

### 小结

k-近邻算法是分类数据最简单最有效的算法，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用可能非常耗时。**k决策树**是其优化版本，可以节省大量的计算开销。

另一个却显示它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。使用**概率测量方法**可以解决这个问题。

## 第 3 章 决策树

决策树的主要优势在于数据形式非常容易理解。决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。

+ 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
+ 缺点：可能会产生过度匹配问题。
+ 适用数据类型：数值型和标称型。

在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。

### 决策树的一般流程

1. 收集数据：可以使用任何方法
2. 准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
3. 分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期
4. 训练算法：构造树的数据结构
5. 测试算法：使用经验树计算错误率
6. 使用算法：此步骤可以适用于任何监督算法，而使用决策树可以更好地理解数据的内在含义

这里使用 [ID3](http://en.wikipedia.org/wiki/ID3) 算法划分数据集

划分数据集的大原则是：将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息。**信息增益(information gain)**和**熵(entropy)**

代码在 `tree.py` 中

熵越高，则混合的数据也越多。另一个度量集合无序程度的方法是基尼不纯度(Gini impurity)，简单地说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率。

抽取不同的特征，根据剩下特征熵的大小，来决定哪个特征是最重要的，然后递归生成决策树。

然后就是用 Matplotlib 把树画出来

### 小结

决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。

ID3 算法可以用于划分标称型数据集。构造决策树时，我们通常采用递归的方法将数据集转化为决策树。

决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配的问题。

## 第 4 章 基于概率论的分类方法：朴素贝叶斯

概率论是许多机器学习算法的基础，所以深刻理解这一主题就显得十分重要。

+ 优点：在数据较少的情况下仍然有效，可以处理多类问题
+ 缺点：对于输入数据的准备方式较为敏感
+ 适用数据类型：标称型数据

机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是用于文档分类的常用算法。

### 朴素贝叶斯的一般过程

1. 收集数据：可以使用任何方法。这里使用 RSS 源
2. 准备数据：需要数值型或者布尔型数据
3. 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好
4. 训练算法：计算不同的独立特征的条件概率
5. 测试算法：计算错误率
6. 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本

朴素贝叶斯的两个假设：

+ 一个特征或者单词出现的可能性与它和其他单词相邻没有关系。
+ 每个特征同等重要

看起来有点不合理，但是这也就是为什么要称其为“朴素”的原因。

朴素贝叶斯分类器通常有两种实现方式：一种基于伯努利模型实现，一种基于多项式模型实现。这里采用第一种实现方式。该实现方式中兵不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的

利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率。如果其中一个概率值为0，那么最后乘积也为0。为了降低这种影响，可以将所有词出现数字初始化为1，并将分母初始化为2

另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的，这里取对数，就可以把乘法变为加法，并且对最后结果没有影响。

### 文档词袋模型

目前为止，我们将每个词的出现与否作为一个特征，这可以被描述为**词集模型(set-of-words model)**。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这个方法被称为**词袋模型(bag-of-words model)**。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。

### 过滤垃圾电子邮件

准备数据：切分文本。去掉标点符号，统一大小写

随机选择数据的一部分作为训练集，而剩余部分作为测试集的过程称为**留存交叉验证(hold-out cross validation)**。为了更精确地估计分类器的错误率，就应该进行多次迭代后求出平均错误率。

移除停止词会降低分类错误率

### 小结

对于分类来说，使用概率有时要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计位置概率的有效方法。

词袋模型在解决文档分类问题上比词集模型有所提高。

## 第 5 章 Logistic 回归

假设现在有一些数据点，我们用一条直线对这些点进行拟合(该线称为最佳拟合直线)，这个拟合过程就称作回归。利用 Logistic 回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。

### Logitstic 回归的一般过程

1. 收集数据：采用任意方法收集数据
